{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB GenAI - LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI GPT API Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a simple chatbot that can answer basic questions about a given topic (e.g., history, technology).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from openai import OpenAI\n",
    "\n",
    "# Create an OpenAI client instance\n",
    "client = OpenAI(api_key=\"FgT3BlbkL34zhz0kHG\")\n",
    "\n",
    "# Define the initial prompt\n",
    "initial_prompt = \"I am a chatbot that can answer basic questions about history. Ask me anything!\"\n",
    "\n",
    "# Function to generate a response\n",
    "def generate_response(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Rate limiting variables\n",
    "last_request_time = time.time()\n",
    "request_interval = 60  # 1 minute (adjust as needed)\n",
    "\n",
    "# Main loop\n",
    "messages = [{\"role\": \"system\", \"content\": initial_prompt}]\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        break\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Rate limiting logic\n",
    "    current_time = time.time()\n",
    "    elapsed_time = current_time - last_request_time\n",
    "    if elapsed_time < request_interval:\n",
    "        delay = request_interval - elapsed_time\n",
    "        print(f\"Rate limit reached. Waiting for {delay:.2f} seconds...\")\n",
    "        time.sleep(delay)\n",
    "    last_request_time = current_time\n",
    "\n",
    "    response = generate_response(messages)\n",
    "    print(f\"AI: {response}\")\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Write a script that takes a long text input and summarizes it into a few sentences.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = \"FgT3BlbkL34zhz0kHG\"\n",
    "\n",
    "# Function to summarize text\n",
    "def summarize_text(text, params):\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=f\"Summarize this text in a few sentences:\\n\\n{text}\",\n",
    "        **params\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example long text\n",
    "long_text = \"\"\"\n",
    "This is a long text about the history of artificial intelligence. Artificial intelligence (AI) is a branch of computer science that aims to create intelligent machines that can think and learn like humans. The idea of AI has been around since ancient times, but it wasn't until the mid-20th century that the field began to take shape.\n",
    "\n",
    "One of the earliest pioneers of AI was Alan Turing, a British mathematician and computer scientist. In 1950, he proposed the Turing Test, which is a way to determine if a machine can exhibit intelligent behavior equivalent to that of a human. The test involves a human evaluator engaging in a natural language conversation with a computer system and a human, without knowing which is which. If the evaluator cannot reliably distinguish between the two, the machine is considered to have passed the test.\n",
    "\n",
    "In the following decades, AI research focused on developing algorithms and techniques for problem-solving, knowledge representation, and machine learning. Some of the early successes of AI include expert systems, which were designed to mimic the decision-making abilities of human experts in specific domains, and game-playing programs like Deep Blue, which famously defeated world chess champion Garry Kasparov in 1997.\n",
    "\n",
    "Today, AI has become increasingly sophisticated and is being applied in a wide range of fields, from natural language processing and computer vision to robotics and healthcare. Deep learning, a subset of machine learning that uses artificial neural networks to learn from data, has been particularly successful in areas such as image and speech recognition, language translation, and recommendation systems.\n",
    "\n",
    "Despite these advancements, there are still many challenges and limitations in AI, such as the lack of general intelligence, the need for large amounts of training data, and the potential for biases and ethical concerns. Researchers and developers continue to work on addressing these issues and pushing the boundaries of what is possible with AI.\n",
    "\"\"\"\n",
    "\n",
    "# Set the initial parameters\n",
    "params = {\n",
    "    \"max_tokens\": 100,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 1.0,\n",
    "    \"frequency_penalty\": 0.0,\n",
    "    \"presence_penalty\": 0.0,\n",
    "    \"best_of\": 1,\n",
    "    \"logprobs\": None\n",
    "}\n",
    "\n",
    "# Generate the summary\n",
    "summary = summarize_text(long_text, params)\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Develop a tool that translates text from one language to another using the API.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai.api_key = os.environ.get(\"FgT3BlbkL34zhz0kHG\")\n",
    "\n",
    "def translate_text(text, source_lang, target_lang, temperature=0.5, max_tokens=200, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, echo=False, logit_bias=None):\n",
    "    \"\"\"\n",
    "    Translates text from one language to another using the OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to be translated.\n",
    "        source_lang (str): The source language code (e.g., \"en\" for English).\n",
    "        target_lang (str): The target language code (e.g., \"fr\" for French).\n",
    "        temperature (float): Controls the randomness of the output. Higher values (up to 1.0) make the output more random, while lower values make it more deterministic.\n",
    "        max_tokens (int): The maximum number of tokens (words or subwords) to generate in the response.\n",
    "        top_p (float): Nucleus sampling parameter that controls the diversity of the output. A value of 1.0 considers all possible tokens at each step, while a smaller value (e.g., 0.9) only considers the most probable tokens.\n",
    "        frequency_penalty (float): A value between 0 and 1 that penalizes new tokens based on their existing frequency in the text so far.\n",
    "        presence_penalty (float): A value between 0 and 1 that penalizes new tokens based on whether they appear in the text so far.\n",
    "        echo (bool): If True, the original text is included in the response.\n",
    "        logit_bias (dict): A dictionary of logit bias values to adjust the model's likelihood of generating particular tokens.\n",
    "    \n",
    "    Returns:\n",
    "        str: The translated text.\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": f\"Translate the following text from {source_lang} to {target_lang}:\"},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        logit_bias=logit_bias\n",
    "    )\n",
    "    \n",
    "    if echo:\n",
    "        return f\"Original text ({source_lang}): {text}\\n\\nTranslated text ({target_lang}): {response.choices[0].message.content.strip()}\"\n",
    "    else:\n",
    "        return response.choices[0].message.content.strip()\n",
    "\n",
    "# Translate English text to French\n",
    "english_text = \"Hello, how are you?\"\n",
    "french_translation = translate_text(english_text, \"en\", \"fr\")\n",
    "print(french_translation)\n",
    "\n",
    "# Translate French text to English\n",
    "french_text = \"Bonjour, comment allez-vous?\"\n",
    "english_translation = translate_text(french_text, \"fr\", \"en\")\n",
    "print(english_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool that determines the sentiment of a given text (positive, negative, neutral).  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "openai.api_key = \"FgT3BlbkL34zhz0kHG\"\n",
    "\n",
    "def sentiment_analysis(text):\n",
    "    # Define the prompt for sentiment analysis\n",
    "    prompt = f\"Analyze the sentiment of the following text: '{text}'\\n\\nSentiment:\"\n",
    "\n",
    "    # Generate the sentiment analysis using the OpenAI API\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=50,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "    # Extract the sentiment from the response\n",
    "    sentiment = response.choices[0].text.strip()\n",
    "\n",
    "    return sentiment\n",
    "\n",
    "# Example usage\n",
    "text = \"I had a great time at the concert last night! The music was amazing, and the energy was incredible.\"\n",
    "sentiment = sentiment_analysis(text)\n",
    "print(f\"Sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Create a text completion application that generates text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "# Set up the OpenAI API key\n",
    "openai.api_key = os.environ.get(\"FgT3BlbkL34zhz0kHGCv17Zy\")\n",
    "\n",
    "def text_completion(prompt, temperature=0.7, max_tokens=100, top_p=1.0, frequency_penalty=0.0, presence_penalty=0.0, stop=None, best_of=1):\n",
    "    \"\"\"\n",
    "    Generates text based on an initial prompt using the OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The initial prompt for text generation.\n",
    "        temperature (float): Controls the randomness of the output. Higher values (up to 1.0) make the output more random, while lower values make it more deterministic.\n",
    "        max_tokens (int): The maximum number of tokens (words or subwords) to generate in the response.\n",
    "        top_p (float): Nucleus sampling parameter that controls the diversity of the output. A value of 1.0 considers all possible tokens at each step, while a smaller value (e.g., 0.9) only considers the most probable tokens.\n",
    "        frequency_penalty (float): A value between 0 and 1 that penalizes new tokens based on their existing frequency in the text so far.\n",
    "        presence_penalty (float): A value between 0 and 1 that penalizes new tokens based on whether they appear in the text so far.\n",
    "        stop (list or str): A list or string of tokens to stop the generation after.\n",
    "        best_of (int): The number of completions to generate for each prompt, and the best one will be returned.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated text based on the initial prompt.\n",
    "    \"\"\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",\n",
    "        prompt=prompt,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "        top_p=top_p,\n",
    "        frequency_penalty=frequency_penalty,\n",
    "        presence_penalty=presence_penalty,\n",
    "        stop=stop,\n",
    "        best_of=best_of\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Example usage\n",
    "initial_prompt = \"Once upon a time, in a faraway land, there lived a\"\n",
    "\n",
    "# Default parameters\n",
    "default_completion = text_completion(initial_prompt)\n",
    "print(\"Default parameters:\\n\", default_completion)\n",
    "\n",
    "# Change temperature\n",
    "creative_completion = text_completion(initial_prompt, temperature=0.9)\n",
    "print(\"\\nTemperature = 0.9:\\n\", creative_completion)\n",
    "\n",
    "# Change max_tokens\n",
    "short_completion = text_completion(initial_prompt, max_tokens=50)\n",
    "print(\"\\nmax_tokens = 50:\\n\", short_completion)\n",
    "\n",
    "# Change top_p\n",
    "diverse_completion = text_completion(initial_prompt, top_p=0.9)\n",
    "print(\"\\ntop_p = 0.9:\\n\", diverse_completion)\n",
    "\n",
    "# Change frequency_penalty and presence_penalty\n",
    "diverse_completion = text_completion(initial_prompt, frequency_penalty=0.5, presence_penalty=0.5)\n",
    "print(\"\\nfrequency_penalty = 0.5, presence_penalty = 0.5:\\n\", diverse_completion)\n",
    "\n",
    "# Change stop\n",
    "stop_completion = text_completion(initial_prompt, stop=[\"!\"])\n",
    "print(\"\\nstop = ['!']:\\n\", stop_completion)\n",
    "\n",
    "# Change best_of\n",
    "best_completion = text_completion(initial_prompt, best_of=3)\n",
    "print(\"\\nbest_of = 3:\\n\", best_completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BONUS: Google Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Conversation\n",
    "**Exercise:** Create a basic chatbot using Google Vertex AI to answer questions about a given topic.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `stop`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import dialogflow\n",
    "\n",
    "# Set up the Dialogflow client\n",
    "project_id = \"2345\"\n",
    "session_id = \"6789\"\n",
    "language_code = \"en-US\"\n",
    "session_client = dialogflow.SessionsClient()\n",
    "\n",
    "# Define the query parameters\n",
    "query_params = dialogflow.QueryParameters(\n",
    "    temperature=0.7,\n",
    "    max_output_tokens=100,\n",
    "    top_p=0.9,\n",
    "    frequency_penalty=0.5,\n",
    "    presence_penalty=0.5,\n",
    "    n=1,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "# Send a query to the chatbot\n",
    "query_input = dialogflow.TextInput(text=\"What is the capital of France?\", language_code=language_code)\n",
    "response = session_client.detect_intent(\n",
    "    request={\n",
    "        \"session\": session_client.session_path(project_id, session_id),\n",
    "        \"query_input\": query_input,\n",
    "        \"query_params\": query_params,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Print the chatbot's response\n",
    "print(f\"Chatbot: {response.query_result.response_message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Summarization\n",
    "**Exercise:** Develop a script that summarizes long text inputs using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `best_of`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "# Set up the Language client\n",
    "client = language_v1.LanguageServiceClient()\n",
    "\n",
    "def summarize_text(text, max_tokens=100, temperature=0.7, top_p=0.9, frequency_penalty=0.5, presence_penalty=0.5, best_of=1, logprobs=False):\n",
    "    # Define the request parameters\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "    request = {\n",
    "        \"document\": {\"content\": text, \"type_\": language_v1.Document.Type.PLAIN_TEXT},\n",
    "        \"encoding_type\": encoding_type,\n",
    "        \"summary_max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"best_of\": best_of,\n",
    "        \"logprobs\": logprobs,\n",
    "    }\n",
    "\n",
    "    # Send the summarization request\n",
    "    response = client.summarize_text(request=request)\n",
    "\n",
    "    # Extract the summary from the response\n",
    "    summary = response.summary\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "long_text = \"Insert your long text here...\"\n",
    "summary = summarize_text(long_text)\n",
    "print(f\"Summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Translation\n",
    "**Exercise:** Create a tool that translates text from one language to another using Google Vertex AI.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `echo`, `logit_bias`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate_v3\n",
    "\n",
    "# Set up the Translation client\n",
    "client = translate_v3.TranslationServiceClient()\n",
    "\n",
    "def translate_text(text, source_lang, target_lang, temperature=0.7, max_output_tokens=100, top_p=0.9, frequency_penalty=0.5, presence_penalty=0.5, echo=False, logit_bias=None):\n",
    "    # Define the request parameters\n",
    "    parent = f\"projects/{client.client_options.project_id}\"\n",
    "    request = {\n",
    "        \"parent\": parent,\n",
    "        \"contents\": [text],\n",
    "        \"mime_type\": \"text/plain\",\n",
    "        \"source_language_code\": source_lang,\n",
    "        \"target_language_code\": target_lang,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": max_output_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"echo\": echo,\n",
    "        \"logit_bias\": logit_bias,\n",
    "    }\n",
    "\n",
    "    # Send the translation request\n",
    "    response = client.translate_text(request=request)\n",
    "\n",
    "    # Extract the translated text from the response\n",
    "    translated_text = response.translations[0].translated_text\n",
    "\n",
    "    return translated_text\n",
    "\n",
    "# Example usage\n",
    "source_text = \"Hello, how are you?\"\n",
    "source_lang = \"en\"\n",
    "target_lang = \"es\"\n",
    "translated_text = translate_text(source_text, source_lang, target_lang)\n",
    "print(f\"Translated text: {translated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "**Exercise:** Implement a sentiment analysis tool using Google Vertex AI to determine the sentiment of a given text.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `n`, `logprobs`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "# Set up the Language client\n",
    "client = language_v1.LanguageServiceClient()\n",
    "\n",
    "def sentiment_analysis(text, temperature=0.7, max_output_tokens=100, top_p=0.9, frequency_penalty=0.5, presence_penalty=0.5, n=1, logprobs=False):\n",
    "    # Define the request parameters\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "    request = {\n",
    "        \"document\": {\"content\": text, \"type_\": language_v1.Document.Type.PLAIN_TEXT},\n",
    "        \"encoding_type\": encoding_type,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": max_output_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"n\": n,\n",
    "        \"logprobs\": logprobs,\n",
    "    }\n",
    "\n",
    "    # Send the sentiment analysis request\n",
    "    response = client.analyze_sentiment(request=request)\n",
    "\n",
    "    # Extract the sentiment score and magnitude from the response\n",
    "    sentiment_score = response.document_sentiment.score\n",
    "    sentiment_magnitude = response.document_sentiment.magnitude\n",
    "\n",
    "    # Determine the sentiment based on the score\n",
    "    if sentiment_score > 0.25:\n",
    "        sentiment = \"Positive\"\n",
    "    elif sentiment_score < -0.25:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "\n",
    "    return sentiment, sentiment_score, sentiment_magnitude\n",
    "\n",
    "# Example usage\n",
    "text = \"I had a great time at the concert last night! The music was amazing, and the energy was incredible.\"\n",
    "sentiment, score, magnitude = sentiment_analysis(text)\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Score: {score}\")\n",
    "print(f\"Magnitude: {magnitude}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Completion\n",
    "**Exercise:** Develop a text completion application using Google Vertex AI to generate text based on an initial prompt.  \n",
    "**Parameters to explore:** `temperature`, `max_output_tokens`, `top_p`, `frequency_penalty`, `presence_penalty`, `stop`, `best_of`.\n",
    "\n",
    "Comment what happen when you change the parameters \n",
    "(read documentation!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import language_v1\n",
    "\n",
    "# Set up the Language client\n",
    "client = language_v1.LanguageServiceClient()\n",
    "\n",
    "def text_completion(prompt, temperature=0.7, max_output_tokens=100, top_p=0.9, frequency_penalty=0.5, presence_penalty=0.5, stop=None, best_of=1):\n",
    "    # Define the request parameters\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "    request = {\n",
    "        \"document\": {\"content\": prompt, \"type_\": language_v1.Document.Type.PLAIN_TEXT},\n",
    "        \"encoding_type\": encoding_type,\n",
    "        \"temperature\": temperature,\n",
    "        \"max_output_tokens\": max_output_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"frequency_penalty\": frequency_penalty,\n",
    "        \"presence_penalty\": presence_penalty,\n",
    "        \"stop\": stop,\n",
    "        \"best_of\": best_of,\n",
    "    }\n",
    "\n",
    "    # Send the text completion request\n",
    "    response = client.complete_text(request=request)\n",
    "\n",
    "    # Extract the generated text from the response\n",
    "    generated_text = response.text\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Example usage\n",
    "initial_prompt = \"Once upon a time, in a faraway land, there lived a brave knight who embarked on a quest to\"\n",
    "generated_text = text_completion(initial_prompt)\n",
    "print(f\"Initial prompt: {initial_prompt}\")\n",
    "print(f\"Generated text: {generated_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
